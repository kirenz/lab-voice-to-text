{
  "hash": "f53534a9f198587e93685fe295d3b340",
  "result": {
    "markdown": "---\ntitle: Speech to Text\ntitle-slide-attributes:\n  data-background-image: ../images/logo.png\n  data-background-size: contain\n  data-background-opacity: \"0.5\"\nlang: en\nsubtitle: Learn how to turn audio into text\nauthor: Jan Kirenz\nexecute:\n  eval: false\n  echo: true\nhighlight-style: github\nformat:\n  revealjs: \n    toc: true\n    toc-depth: 1\n    embed-resources: false\n    theme: [dark, ../custom.scss]  \n    incremental: true\n    transition: slide\n    background-transition: fade\n    transition-speed: slow\n    code-copy: true\n    code-line-numbers: true\n    smaller: false\n    scrollable: true\n    slide-number: c\n    preview-links: auto\n    chalkboard: \n      buttons: false\n   # logo: ../images/logo.png\n    footer: Jan Kirenz\n---\n\n# Introduction\n\nTranscribe audio into whatever language the audio is in.\n\n# Setup\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport openai\nfrom pathlib import Path\nimport difflib\n\nfrom dotenv import load_dotenv, find_dotenv\n_ = load_dotenv(find_dotenv())\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n```\n:::\n\n\n# Simple example\n\n## Create mp3 data {.smaller}\n\n- Use [ttsmp3](https://ttsmp3.com/) to generate your audio data\n- save it as `example.mp3` in your Downloads folder\n\n- Example text to use in [ttsmp3](https://ttsmp3.com/) :\n\n\n```{markdown}\nWelcome to the world of automatic speech recognition with OpenAI's Whisper API. Whisper is an automatic speech recognition system trained on a massive amount of multilingual and multitask supervised data collected from the web. It is designed to convert spoken language into written text, making transcription tasks a breeze.\n\n```\n\n\n\n## SSML {.smaller}\n\n- You may use [Speech Synthesis Markup Language (SSML)](https://docs.aws.amazon.com/polly/latest/dg/supportedtags.html) to customize the output\n\n\n```{markdown}\n- Add a **break**: Mary had a little lamb <break time=\"1s\"/> Whose fleece was white as snow.\n- **Emphasizing**: I already told you I <emphasis level=\"strong\">really like </emphasis> that person.\n- **Speed**: For dramatic purposes, you might wish to <prosody rate=\"slow\">slow down the speaking rate of your text.</prosody>\nOr if you are in a hurry <prosody rate=\"fast\">your may want to speed it up a bit.</prosody>\n- **Pitch**: Do you like sythesized speech <prosody pitch=\"high\">with a pitch that is higher than normal?</prosody> Or do you prefer your speech <prosody pitch=\"-20%\">with a somewhat lower pitch?</prosody>\n- **Whisper** <amazon:effect name=\"whispered\">If you make any noise, </amazon:effect> she said, <amazon:effect name=\"whispered\">they will hear us.</amazon:effect>\n- **Conversations**: It is possible to switch between speakers within the text:\n  - [speaker:Brian] Hello Emma\n  - [speaker:Emma] Hey Brian\n```\n\n\n## Load data\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\npath_to_example = Path.home() / 'Downloads/example.mp3' \n```\n:::\n\n\n. . .\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\naudio_file= open(path_to_example, \"rb\")\n```\n:::\n\n\n- 'r': Read mode. File is opened for reading. \n- 'b': Binary mode. File is opened in binary mode, which is used for non-text files (e.g., image and sound files).\n\n## Transcribe\n\nFile uploads are currently limited to 25 MB and the following input file types are supported: mp3, mp4, mpeg, mpga, m4a, wav, and webm.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ntranscript = openai.Audio.transcribe(\"whisper-1\", audio_file)\ntranscript\n```\n:::\n\n\n. . .\n\n\n```{bash}\n<OpenAIObject at 0x117dea450> JSON: {\n  \"text\": \"Welcome to the world of Automatic Speech Recognition  \\\n  with OpenAI's Whisper API. Whisper is an automatic speech  \\\n  recognition system trained on a massive amount of multilingual \\\n  and multitask supervised data collected from the web. It is designed \\\n  to convert spoken language into written text, making transcription tasks a breeze.\"\n}\n\n```\n\n\n\n# Extract only text\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\ntext_output = transcript['text']\ntext_output\n```\n:::\n\n\n- Output:\n\n. . .\n\n\n```{markdown}\n\"Welcome to the world of Automatic Speech Recognition with OpenAI's Whisper API. Whisper is an automatic speech recognition system trained on a massive amount of multilingual and multitask supervised data collected from the web. It is designed to convert spoken language into written text, making transcription tasks a breeze.\"\n\n```\n\n\n# Save text\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nwrite_to_output = Path.home() / 'Downloads/output.txt' \n```\n:::\n\n\n. . .\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nwith open(write_to_output, 'w') as file:\n    file.write(text_output)\n```\n:::\n\n\n# Workflow for multiple files\n\n- We will use an example with multiple .mp3 files which are stored in the 'Downloads' folder\n\n- Note that the files will be renamed during the process\n\n## Prepare environment\n\n- Create a new folder called `data` in your Downloads folder\n- Place at least two mp3 files in your `data` folder\n\n## Obtain file names\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\ndownloads_folder = Path.home() / 'Downloads/data'\n\nmp3_files = [f for f in downloads_folder.iterdir() if f.suffix == '.mp3']\n```\n:::\n\n\n## Obtain time of storage \n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\n# Get file paths along with their stored time\nfile_details = [(f, f.stat().st_mtime) for f in mp3_files]\n```\n:::\n\n\n## Sort files according to time\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nsorted_files = sorted(file_details, key=lambda x: x[1])\n```\n:::\n\n\n## Move and rename files\n\n- Move the files to a folder of your choice (we use our data folder) and rename the files to 01_audio, 02_audio, etc.\n\n. . .\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\ndestination_folder = Path.home() / 'Downloads/data'\n\nfor idx, (file_path, _) in enumerate(sorted_files):\n    dest_path = destination_folder / f\"{str(idx+1).zfill(2)}_audio.mp3\"\n    file_path.rename(dest_path)\n\n```\n:::\n\n\n## Transcripe helper function\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\ndef transcribe_audio(audio_file_path):\n    \"\"\"\n    Transcribe the given audio file using OpenAI's Whisper ASR system.\n    \"\"\"\n    audio_file_path = Path(audio_file_path)  # Convert to Path object if it's not already\n    with audio_file_path.open(\"rb\") as audio_file:\n        response = openai.Audio.transcribe(\"whisper-1\", audio_file)\n    return response[\"text\"]\n```\n:::\n\n\n## Apply function\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\ndata_folder = destination_folder\n\n# Get a list of all audio files in the /data/ folder\naudio_files = sorted(data_folder.glob('*.mp3'))\n\nfor audio_file in audio_files:\n    # Transcribe the audio file\n    transcription = transcribe_audio(audio_file)\n\n    # Create a new txt file name based on the audio file's name\n    txt_file_name = audio_file.stem + \".txt\"\n    txt_file_path = data_folder / txt_file_name\n\n    # Save the transcription to the txt file\n    with txt_file_path.open('w') as txt_file:\n        txt_file.write(transcription)\n```\n:::\n\n\n## Save as markdown\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nmarkdown_content = []\n\n# Iterate through all txt files in the /data/ folder\ntxt_files = sorted(data_folder.glob('*.txt'))\n\nfor txt_file in txt_files:\n    # Read the content of the txt file\n    with txt_file.open('r') as file:\n        file_content = file.read()\n    \n    # Append the file name as a headline and then the content to the markdown content list\n    markdown_content.append(f\"# {txt_file.name}\\n\\n{file_content}\\n\")\n\n# Join all the markdown content pieces to create the final document\nfinal_markdown = \"\\n\".join(markdown_content)\n\n# Save the combined content to a markdown file\noutput_file = data_folder / \"combined_audio_transcriptions.md\"\nwith output_file.open('w') as file:\n    file.write(final_markdown)\n```\n:::\n\n\n# Post processing with GPT\n\n## System prompt\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\nsystem_prompt = \"You are a helpful assistant. \\\n    Your task is to correct any spelling discrepancies in the \\\n    transcribed text. Only add necessary punctuation such as \\\n    periods, commas, and capitalization, and use only the context provided\"\n```\n:::\n\n\n## Helper function\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\ndef generate_corrected_transcript(temperature, system_prompt, transcribed_text):\n    response = openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo\",\n        temperature=temperature,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": system_prompt\n            },\n            {\n                \"role\": \"user\",\n                \"content\": transcribed_text\n            }\n        ]\n    )\n    return response['choices'][0]['message']['content']\n```\n:::\n\n\n## Correct data for multiple files {.smaller}\n\n- In the appendix is an example of how to correct a single file\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\n# Directory containing the transcription files\ndata_folder = destination_folder\n\n# List to hold the corrected transcriptions\ncorrected_transcriptions = []\n\n# Iterate through each individual .txt file\nfor txt_file_path in sorted(data_folder.glob('??_audio.txt')):\n    # Read the transcription from the file\n    with txt_file_path.open('r') as txt_file:\n        transcription = txt_file.read()\n\n    # Post-process the transcription using GPT-4\n    corrected_text = generate_corrected_transcript(0.5, system_prompt, transcription)\n    \n    # Append the file name as a headline and then the corrected content to the list\n    corrected_transcriptions.append(f\"# {txt_file_path.name}\\n\\n{corrected_text}\\n\")\n\n# Combine the corrected transcriptions and save them to the new markdown file\noutput_file = data_folder / \"corrected_text.md\"\nwith output_file.open('w') as out_file:\n    out_file.write(\"\\n\".join(corrected_transcriptions))\n```\n:::\n\n\n## Show differences {.smaller}\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\n# Define file paths\noriginal_file_path = Path.home() / 'Downloads/data/combined_audio_transcriptions.md' \n\ncorrected_file_path = Path.home() / 'Downloads/data/corrected_text.md'\n\n# Read the content of both markdown files\nwith original_file_path.open('r') as f:\n    original_content = f.readlines()\n\nwith corrected_file_path.open('r') as f:\n    corrected_content = f.readlines()\n\n# Generate a side-by-side comparison in HTML format\ndiffer = difflib.HtmlDiff()\ncomparison_html = differ.make_file(original_content, corrected_content)\n\n# Add custom CSS for content wrapping and better visualization\ncustom_css = \"\"\"\n<style>\n    body {\n        font-family: Arial, sans-serif;\n        margin: 20px;\n    }\n    table.diff {\n        width: 100%;\n        border-collapse: collapse;\n    }\n    td {\n        vertical-align: top;\n        padding: 5px;\n        border: 1px solid #ddd;\n        white-space: pre-wrap;\n        word-break: break-word;\n        overflow-wrap: break-word;\n    }\n</style>\n\"\"\"\n\n# The rest of your code remains the same...\n\n\n# Insert the custom CSS into the generated HTML content\ncomparison_html = comparison_html.replace('<head>', '<head>' + custom_css)\n\n\n# Save the HTML comparison to a file\noutput_html_path = Path.home() / 'Downloads/data/comparison.html'\nwith output_html_path.open('w') as f:\n    f.write(comparison_html)\n```\n:::\n\n\n# What's next? {background-image=\"../images/logo.png\" background-opacity=\"0.5\"}\n\n**Congratulations! You have completed this tutorial** üëç\n\n**Next, you may want to go back to the [lab's website](https://kirenz.github.io/lab-voice-to-text/)**\n\n\n# Appendix\n\n## Correct data for a single file\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\n# Read the transcriptions from the markdown file\nmarkdown_path = Path('YOUR/PATH/DATA.md')\nwith markdown_path.open('r') as md_file:\n    transcriptions = md_file.read()\n\n# Post-process the transcriptions using GPT-4\ncorrected_text = generate_corrected_transcript(0.2, system_prompt, transcriptions)\n\n# save the corrected text to a new markdown file\noutput_path = Path('YOUR/PATH/corrected_data.md')\nwith output_path.open('w') as out_file:\n    out_file.write(corrected_text)\n```\n:::\n\n\n",
    "supporting": [
      "voice-to-text_files"
    ],
    "filters": [],
    "includes": {}
  }
}