{
  "hash": "d7efc822135c8a89706d6063dfa766fe",
  "result": {
    "markdown": "---\ntitle: Speech to Text\ntitle-slide-attributes:\n  data-background-image: ../images/logo.png\n  data-background-size: contain\n  data-background-opacity: \"0.5\"\nlang: en\nsubtitle: Learn how to turn audio into text\nauthor: Jan Kirenz\nexecute:\n  eval: false\n  echo: true\nhighlight-style: github\nformat:\n  revealjs: \n    toc: true\n    toc-depth: 1\n    embed-resources: false\n    theme: [dark, ../custom.scss]  \n    incremental: true\n    transition: slide\n    background-transition: fade\n    transition-speed: slow\n    code-copy: true\n    code-line-numbers: true\n    smaller: false\n    scrollable: true\n    slide-number: c\n    preview-links: auto\n    chalkboard: \n      buttons: false\n   # logo: ../images/logo.png\n    footer: Jan Kirenz\n---\n\n# Introduction\n\nTranscribe audio into whatever language the audio is in.\n\n# Setup\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport openai\nfrom pathlib import Path\nimport difflib\n\nfrom dotenv import load_dotenv, find_dotenv\n_ = load_dotenv(find_dotenv())\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n```\n:::\n\n\n# Simple example\n\n## Create mp3 data {.smaller}\n\n- Use [ttsmp3](https://ttsmp3.com/) to generate your audio data\n- save it as `example.mp3` in your Downloads folder\n\n- Example text to use in [ttsmp3](https://ttsmp3.com/) :\n\n\n```{markdown}\nWelcome to the world of automatic speech recognition with OpenAI's Whisper API. Whisper is an automatic speech recognition system trained on a massive amount of multilingual and multitask supervised data collected from the web. It is designed to convert spoken language into written text, making transcription tasks a breeze.\n\n```\n\n\n\n## SSML {.smaller}\n\n- You may use [Speech Synthesis Markup Language (SSML)](https://docs.aws.amazon.com/polly/latest/dg/supportedtags.html) to customize the output\n\n- Add a **break**: Mary had a little lamb <break time=\"1s\"/> Whose fleece was white as snow.\n- **Emphasizing** words: I already told you I <emphasis level=\"strong\">really like </emphasis> that person.\n- **Speed**: For dramatic purposes, you might wish to <prosody rate=\"slow\">slow down the speaking rate of your text.</prosody>\nOr if you are in a hurry <prosody rate=\"fast\">your may want to speed it up a bit.</prosody>\n- **Pitch**: Do you like sythesized speech <prosody pitch=\"high\">with a pitch that is higher than normal?</prosody> Or do you prefer your speech <prosody pitch=\"-20%\">with a somewhat lower pitch?</prosody>\n- **Whisper** <amazon:effect name=\"whispered\">If you make any noise, </amazon:effect> she said, <amazon:effect name=\"whispered\">they will hear us.</amazon:effect>\n- **Conversations**: It is possible to switch between speakers within the text. Just use the following format:\n  - [speaker:Brian] Hello Emma\n  - [speaker:Emma] Hey Brian\n  - [speaker:Brian] How are you doing?\n  - [speaker:Emma] I am fine. May i invite you to a cup of tea?\n\n\n## Load data\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\npath_to_example = Path.home() / 'Downloads/example.mp3' \n```\n:::\n\n\n. . .\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\naudio_file= open(path_to_example, \"rb\")\n```\n:::\n\n\n- 'r': Read mode. File is opened for reading. \n- 'b': Binary mode. File is opened in binary mode, which is used for non-text files (e.g., image and sound files).\n\n## Transcribe\n\nFile uploads are currently limited to 25 MB and the following input file types are supported: mp3, mp4, mpeg, mpga, m4a, wav, and webm.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ntranscript = openai.Audio.transcribe(\"whisper-1\", audio_file)\ntranscript\n```\n:::\n\n\n. . .\n\n\n```{bash}\n<OpenAIObject at 0x117dea450> JSON: {\n  \"text\": \"Welcome to the world of Automatic Speech Recognition with OpenAI's Whisper API. Whisper is an automatic speech recognition system trained on a massive amount of multilingual and multitask supervised data collected from the web. It is designed to convert spoken language into written text, making transcription tasks a breeze.\"\n}\n\n```\n\n\n\n# Extract only text\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\ntext_output = transcript['text']\ntext_output\n```\n:::\n\n\n- Output:\n\n. . .\n\n\n```{markdown}\n\"Welcome to the world of Automatic Speech Recognition with OpenAI's Whisper API. Whisper is an automatic speech recognition system trained on a massive amount of multilingual and multitask supervised data collected from the web. It is designed to convert spoken language into written text, making transcription tasks a breeze.\"\n\n```\n\n\n# Save text\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nwrite_to_output = Path.home() / 'Downloads/output.txt' \n```\n:::\n\n\n. . .\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nwith open(write_to_output, 'w') as file:\n    file.write(text_output)\n```\n:::\n\n\n# Workflow for multiple files\n\n- We will use an example with multiple .mp3 files which are stored in the 'Downloads' folder\n\n- Note that the files will be renamed during the process\n\n## Prepare environment\n\n- Create a new folder called `data` in your Downloads folder\n- Place at least two mp3 files in your `data` folder\n\n## Obtain file names\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\ndownloads_folder = Path.home() / 'Downloads/data'\n\nmp3_files = [f for f in downloads_folder.iterdir() if f.suffix == '.mp3']\n```\n:::\n\n\n## Obtain time of storage \n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\n# Get file paths along with their stored time\nfile_details = [(f, f.stat().st_mtime) for f in mp3_files]\n```\n:::\n\n\n## Sort files according to time\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nsorted_files = sorted(file_details, key=lambda x: x[1])\n```\n:::\n\n\n## Move and rename files\n\n- Move the files to a folder of your choice (we use our data folder) and rename the files to 01_audio, 02_audio, etc.\n\n. . .\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\ndestination_folder = Path.home() / 'Downloads/data'\n\nfor idx, (file_path, _) in enumerate(sorted_files):\n    dest_path = destination_folder / f\"{str(idx+1).zfill(2)}_audio.mp3\"\n    file_path.rename(dest_path)\n\n```\n:::\n\n\n## Transcripe helper function\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\ndef transcribe_audio(audio_file_path):\n    \"\"\"\n    Transcribe the given audio file using OpenAI's Whisper ASR system.\n    \"\"\"\n    audio_file_path = Path(audio_file_path)  # Convert to Path object if it's not already\n    with audio_file_path.open(\"rb\") as audio_file:\n        response = openai.Audio.transcribe(\"whisper-1\", audio_file)\n    return response[\"text\"]\n```\n:::\n\n\n## Apply function\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\ndata_folder = destination_folder\n\n# Get a list of all audio files in the /data/ folder\naudio_files = sorted(data_folder.glob('*.mp3'))\n\nfor audio_file in audio_files:\n    # Transcribe the audio file\n    transcription = transcribe_audio(audio_file)\n\n    # Create a new txt file name based on the audio file's name\n    txt_file_name = audio_file.stem + \".txt\"\n    txt_file_path = data_folder / txt_file_name\n\n    # Save the transcription to the txt file\n    with txt_file_path.open('w') as txt_file:\n        txt_file.write(transcription)\n```\n:::\n\n\n## Save as markdown\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nmarkdown_content = []\n\n# Iterate through all txt files in the /data/ folder\ntxt_files = sorted(data_folder.glob('*.txt'))\n\nfor txt_file in txt_files:\n    # Read the content of the txt file\n    with txt_file.open('r') as file:\n        file_content = file.read()\n    \n    # Append the file name as a headline and then the content to the markdown content list\n    markdown_content.append(f\"# {txt_file.name}\\n\\n{file_content}\\n\")\n\n# Join all the markdown content pieces to create the final document\nfinal_markdown = \"\\n\".join(markdown_content)\n\n# Save the combined content to a markdown file\noutput_file = data_folder / \"combined_audio_transcriptions.md\"\nwith output_file.open('w') as file:\n    file.write(final_markdown)\n```\n:::\n\n\n# Post processing with GPT\n\n## System prompt\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\nsystem_prompt = \"You are a helpful assistant. Your task is to correct any spelling discrepancies in the transcribed text. Only add necessary punctuation such as periods, commas, and capitalization, and use only the context provided\"\n```\n:::\n\n\n## Helper function\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\ndef generate_corrected_transcript(temperature, system_prompt, transcribed_text):\n    response = openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo\",\n        temperature=temperature,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": system_prompt\n            },\n            {\n                \"role\": \"user\",\n                \"content\": transcribed_text\n            }\n        ]\n    )\n    return response['choices'][0]['message']['content']\n```\n:::\n\n\n## Correct data for multiple files\n\n- In the appendix is an example of how to correct a single file\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\n# Directory containing the transcription files\ndata_folder = destination_folder\n\n# List to hold the corrected transcriptions\ncorrected_transcriptions = []\n\n# Iterate through each individual .txt file\nfor txt_file_path in sorted(data_folder.glob('??_audio.txt')):\n    # Read the transcription from the file\n    with txt_file_path.open('r') as txt_file:\n        transcription = txt_file.read()\n\n    # Post-process the transcription using GPT-4\n    corrected_text = generate_corrected_transcript(0.5, system_prompt, transcription)\n    \n    # Append the file name as a headline and then the corrected content to the list\n    corrected_transcriptions.append(f\"# {txt_file_path.name}\\n\\n{corrected_text}\\n\")\n\n# Combine the corrected transcriptions and save them to the new markdown file\noutput_file = data_folder / \"corrected_text.md\"\nwith output_file.open('w') as out_file:\n    out_file.write(\"\\n\".join(corrected_transcriptions))\n```\n:::\n\n\n## Show differences\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\n# Define file paths\noriginal_file_path = Path.home() / 'Downloads/data/combined_audio_transcriptions.md' \n\ncorrected_file_path = Path.home() / 'Downloads/data/corrected_text.md'\n\n# Read the content of both markdown files\nwith original_file_path.open('r') as f:\n    original_content = f.readlines()\n\nwith corrected_file_path.open('r') as f:\n    corrected_content = f.readlines()\n\n# Generate a side-by-side comparison in HTML format\ndiffer = difflib.HtmlDiff()\ncomparison_html = differ.make_file(original_content, corrected_content)\n\n# Add custom CSS for content wrapping and better visualization\ncustom_css = \"\"\"\n<style>\n    body {\n        font-family: Arial, sans-serif;\n        margin: 20px;\n    }\n    table.diff {\n        width: 100%;\n        border-collapse: collapse;\n    }\n    td {\n        vertical-align: top;\n        padding: 5px;\n        border: 1px solid #ddd;\n        white-space: pre-wrap;\n        word-break: break-word;\n        overflow-wrap: break-word;\n    }\n</style>\n\"\"\"\n\n# The rest of your code remains the same...\n\n\n# Insert the custom CSS into the generated HTML content\ncomparison_html = comparison_html.replace('<head>', '<head>' + custom_css)\n\n\n# Save the HTML comparison to a file\noutput_html_path = Path.home() / 'Downloads/data/comparison.html'\nwith output_html_path.open('w') as f:\n    f.write(comparison_html)\n```\n:::\n\n\n# What's next? {background-image=\"../images/logo.png\" background-opacity=\"0.5\"}\n\n**Congratulations! You have completed this tutorial** 👍\n\n**Next, you may want to go back to the [lab's website](https://kirenz.github.io/lab-voice-to-text/)**\n\n\n# Appendix\n\n## Correct data for a single file\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\n# Read the transcriptions from the markdown file\nmarkdown_path = Path('YOUR/PATH/DATA.md')\nwith markdown_path.open('r') as md_file:\n    transcriptions = md_file.read()\n\n# Post-process the transcriptions using GPT-4\ncorrected_text = generate_corrected_transcript(0.2, system_prompt, transcriptions)\n\n# save the corrected text to a new markdown file\noutput_path = Path('YOUR/PATH/corrected_data.md')\nwith output_path.open('w') as out_file:\n    out_file.write(corrected_text)\n```\n:::\n\n\n",
    "supporting": [
      "voice-to-text_files"
    ],
    "filters": [],
    "includes": {}
  }
}