[
  {
    "objectID": "slides/voice-to-text.html#create-mp3-data",
    "href": "slides/voice-to-text.html#create-mp3-data",
    "title": "Speech to Text",
    "section": "Create mp3 data",
    "text": "Create mp3 data\n\nUse ttsmp3 to generate your audio data\nsave it as example.mp3 in your Downloads folder\nExample text to use in ttsmp3 :\n\nWelcome to the world of automatic speech recognition with OpenAI's Whisper API. Whisper is an automatic speech recognition system trained on a massive amount of multilingual and multitask supervised data collected from the web. It is designed to convert spoken language into written text, making transcription tasks a breeze."
  },
  {
    "objectID": "slides/voice-to-text.html#ssml",
    "href": "slides/voice-to-text.html#ssml",
    "title": "Speech to Text",
    "section": "SSML",
    "text": "SSML\n\nYou may use Speech Synthesis Markup Language (SSML) to customize the output\n\n- Add a **break**: Mary had a little lamb &lt;break time=\"1s\"/&gt; Whose fleece was white as snow.\n- **Emphasizing**: I already told you I &lt;emphasis level=\"strong\"&gt;really like &lt;/emphasis&gt; that person.\n- **Speed**: For dramatic purposes, you might wish to &lt;prosody rate=\"slow\"&gt;slow down the speaking rate of your text.&lt;/prosody&gt;\nOr if you are in a hurry &lt;prosody rate=\"fast\"&gt;your may want to speed it up a bit.&lt;/prosody&gt;\n- **Pitch**: Do you like sythesized speech &lt;prosody pitch=\"high\"&gt;with a pitch that is higher than normal?&lt;/prosody&gt; Or do you prefer your speech &lt;prosody pitch=\"-20%\"&gt;with a somewhat lower pitch?&lt;/prosody&gt;\n- **Whisper** &lt;amazon:effect name=\"whispered\"&gt;If you make any noise, &lt;/amazon:effect&gt; she said, &lt;amazon:effect name=\"whispered\"&gt;they will hear us.&lt;/amazon:effect&gt;\n- **Conversations**: It is possible to switch between speakers within the text:\n  - [speaker:Brian] Hello Emma\n  - [speaker:Emma] Hey Brian"
  },
  {
    "objectID": "slides/voice-to-text.html#load-data",
    "href": "slides/voice-to-text.html#load-data",
    "title": "Speech to Text",
    "section": "Load data",
    "text": "Load data\n\npath_to_example = Path.home() / 'Downloads/example.mp3' \n\n\n\naudio_file= open(path_to_example, \"rb\")\n\n\n‘r’: Read mode. File is opened for reading.\n‘b’: Binary mode. File is opened in binary mode, which is used for non-text files (e.g., image and sound files)."
  },
  {
    "objectID": "slides/voice-to-text.html#transcribe",
    "href": "slides/voice-to-text.html#transcribe",
    "title": "Speech to Text",
    "section": "Transcribe",
    "text": "Transcribe\nFile uploads are currently limited to 25 MB and the following input file types are supported: mp3, mp4, mpeg, mpga, m4a, wav, and webm.\n\ntranscript = openai.Audio.transcribe(\"whisper-1\", audio_file)\ntranscript\n\n\n&lt;OpenAIObject at 0x117dea450&gt; JSON: {\n  \"text\": \"Welcome to the world of Automatic Speech Recognition  \\\n  with OpenAI's Whisper API. Whisper is an automatic speech  \\\n  recognition system trained on a massive amount of multilingual \\\n  and multitask supervised data collected from the web. It is designed \\\n  to convert spoken language into written text, making transcription tasks a breeze.\"\n}"
  },
  {
    "objectID": "slides/voice-to-text.html#prepare-environment",
    "href": "slides/voice-to-text.html#prepare-environment",
    "title": "Speech to Text",
    "section": "Prepare environment",
    "text": "Prepare environment\n\nCreate a new folder called data in your Downloads folder\nPlace at least two mp3 files in your data folder"
  },
  {
    "objectID": "slides/voice-to-text.html#obtain-file-names",
    "href": "slides/voice-to-text.html#obtain-file-names",
    "title": "Speech to Text",
    "section": "Obtain file names",
    "text": "Obtain file names\n\ndownloads_folder = Path.home() / 'Downloads/data'\n\nmp3_files = [f for f in downloads_folder.iterdir() if f.suffix == '.mp3']"
  },
  {
    "objectID": "slides/voice-to-text.html#obtain-time-of-storage",
    "href": "slides/voice-to-text.html#obtain-time-of-storage",
    "title": "Speech to Text",
    "section": "Obtain time of storage",
    "text": "Obtain time of storage\n\n# Get file paths along with their stored time\nfile_details = [(f, f.stat().st_mtime) for f in mp3_files]"
  },
  {
    "objectID": "slides/voice-to-text.html#sort-files-according-to-time",
    "href": "slides/voice-to-text.html#sort-files-according-to-time",
    "title": "Speech to Text",
    "section": "Sort files according to time",
    "text": "Sort files according to time\n\nsorted_files = sorted(file_details, key=lambda x: x[1])"
  },
  {
    "objectID": "slides/voice-to-text.html#move-and-rename-files",
    "href": "slides/voice-to-text.html#move-and-rename-files",
    "title": "Speech to Text",
    "section": "Move and rename files",
    "text": "Move and rename files\n\nMove the files to a folder of your choice (we use our data folder) and rename the files to 01_audio, 02_audio, etc.\n\n\n\ndestination_folder = Path.home() / 'Downloads/data'\n\nfor idx, (file_path, _) in enumerate(sorted_files):\n    dest_path = destination_folder / f\"{str(idx+1).zfill(2)}_audio.mp3\"\n    file_path.rename(dest_path)"
  },
  {
    "objectID": "slides/voice-to-text.html#transcripe-helper-function",
    "href": "slides/voice-to-text.html#transcripe-helper-function",
    "title": "Speech to Text",
    "section": "Transcripe helper function",
    "text": "Transcripe helper function\n\ndef transcribe_audio(audio_file_path):\n    \"\"\"\n    Transcribe the given audio file using OpenAI's Whisper ASR system.\n    \"\"\"\n    audio_file_path = Path(audio_file_path)  # Convert to Path object if it's not already\n    with audio_file_path.open(\"rb\") as audio_file:\n        response = openai.Audio.transcribe(\"whisper-1\", audio_file)\n    return response[\"text\"]"
  },
  {
    "objectID": "slides/voice-to-text.html#apply-function",
    "href": "slides/voice-to-text.html#apply-function",
    "title": "Speech to Text",
    "section": "Apply function",
    "text": "Apply function\n\ndata_folder = destination_folder\n\n# Get a list of all audio files in the /data/ folder\naudio_files = sorted(data_folder.glob('*.mp3'))\n\nfor audio_file in audio_files:\n    # Transcribe the audio file\n    transcription = transcribe_audio(audio_file)\n\n    # Create a new txt file name based on the audio file's name\n    txt_file_name = audio_file.stem + \".txt\"\n    txt_file_path = data_folder / txt_file_name\n\n    # Save the transcription to the txt file\n    with txt_file_path.open('w') as txt_file:\n        txt_file.write(transcription)"
  },
  {
    "objectID": "slides/voice-to-text.html#save-as-markdown",
    "href": "slides/voice-to-text.html#save-as-markdown",
    "title": "Speech to Text",
    "section": "Save as markdown",
    "text": "Save as markdown\n\nmarkdown_content = []\n\n# Iterate through all txt files in the /data/ folder\ntxt_files = sorted(data_folder.glob('*.txt'))\n\nfor txt_file in txt_files:\n    # Read the content of the txt file\n    with txt_file.open('r') as file:\n        file_content = file.read()\n    \n    # Append the file name as a headline and then the content to the markdown content list\n    markdown_content.append(f\"# {txt_file.name}\\n\\n{file_content}\\n\")\n\n# Join all the markdown content pieces to create the final document\nfinal_markdown = \"\\n\".join(markdown_content)\n\n# Save the combined content to a markdown file\noutput_file = data_folder / \"combined_audio_transcriptions.md\"\nwith output_file.open('w') as file:\n    file.write(final_markdown)"
  },
  {
    "objectID": "slides/voice-to-text.html#system-prompt",
    "href": "slides/voice-to-text.html#system-prompt",
    "title": "Speech to Text",
    "section": "System prompt",
    "text": "System prompt\n\nsystem_prompt = \"You are a helpful assistant. \\\n    Your task is to correct any spelling discrepancies in the \\\n    transcribed text. Only add necessary punctuation such as \\\n    periods, commas, and capitalization, and use only the context provided\""
  },
  {
    "objectID": "slides/voice-to-text.html#helper-function",
    "href": "slides/voice-to-text.html#helper-function",
    "title": "Speech to Text",
    "section": "Helper function",
    "text": "Helper function\n\ndef generate_corrected_transcript(temperature, system_prompt, transcribed_text):\n    response = openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo\",\n        temperature=temperature,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": system_prompt\n            },\n            {\n                \"role\": \"user\",\n                \"content\": transcribed_text\n            }\n        ]\n    )\n    return response['choices'][0]['message']['content']"
  },
  {
    "objectID": "slides/voice-to-text.html#correct-data-for-multiple-files",
    "href": "slides/voice-to-text.html#correct-data-for-multiple-files",
    "title": "Speech to Text",
    "section": "Correct data for multiple files",
    "text": "Correct data for multiple files\n\nIn the appendix is an example of how to correct a single file\n\n\n# Directory containing the transcription files\ndata_folder = destination_folder\n\n# List to hold the corrected transcriptions\ncorrected_transcriptions = []\n\n# Iterate through each individual .txt file\nfor txt_file_path in sorted(data_folder.glob('??_audio.txt')):\n    # Read the transcription from the file\n    with txt_file_path.open('r') as txt_file:\n        transcription = txt_file.read()\n\n    # Post-process the transcription using GPT-4\n    corrected_text = generate_corrected_transcript(0.5, system_prompt, transcription)\n    \n    # Append the file name as a headline and then the corrected content to the list\n    corrected_transcriptions.append(f\"# {txt_file_path.name}\\n\\n{corrected_text}\\n\")\n\n# Combine the corrected transcriptions and save them to the new markdown file\noutput_file = data_folder / \"corrected_text.md\"\nwith output_file.open('w') as out_file:\n    out_file.write(\"\\n\".join(corrected_transcriptions))"
  },
  {
    "objectID": "slides/voice-to-text.html#show-differences",
    "href": "slides/voice-to-text.html#show-differences",
    "title": "Speech to Text",
    "section": "Show differences",
    "text": "Show differences\n\n# Define file paths\noriginal_file_path = Path.home() / 'Downloads/data/combined_audio_transcriptions.md' \n\ncorrected_file_path = Path.home() / 'Downloads/data/corrected_text.md'\n\n# Read the content of both markdown files\nwith original_file_path.open('r') as f:\n    original_content = f.readlines()\n\nwith corrected_file_path.open('r') as f:\n    corrected_content = f.readlines()\n\n# Generate a side-by-side comparison in HTML format\ndiffer = difflib.HtmlDiff()\ncomparison_html = differ.make_file(original_content, corrected_content)\n\n# Add custom CSS for content wrapping and better visualization\ncustom_css = \"\"\"\n&lt;style&gt;\n    body {\n        font-family: Arial, sans-serif;\n        margin: 20px;\n    }\n    table.diff {\n        width: 100%;\n        border-collapse: collapse;\n    }\n    td {\n        vertical-align: top;\n        padding: 5px;\n        border: 1px solid #ddd;\n        white-space: pre-wrap;\n        word-break: break-word;\n        overflow-wrap: break-word;\n    }\n&lt;/style&gt;\n\"\"\"\n\n# The rest of your code remains the same...\n\n\n# Insert the custom CSS into the generated HTML content\ncomparison_html = comparison_html.replace('&lt;head&gt;', '&lt;head&gt;' + custom_css)\n\n\n# Save the HTML comparison to a file\noutput_html_path = Path.home() / 'Downloads/data/comparison.html'\nwith output_html_path.open('w') as f:\n    f.write(comparison_html)"
  },
  {
    "objectID": "slides/voice-to-text.html#correct-data-for-a-single-file",
    "href": "slides/voice-to-text.html#correct-data-for-a-single-file",
    "title": "Speech to Text",
    "section": "Correct data for a single file",
    "text": "Correct data for a single file\n\n# Read the transcriptions from the markdown file\nmarkdown_path = Path('YOUR/PATH/DATA.md')\nwith markdown_path.open('r') as md_file:\n    transcriptions = md_file.read()\n\n# Post-process the transcriptions using GPT-4\ncorrected_text = generate_corrected_transcript(0.2, system_prompt, transcriptions)\n\n# save the corrected text to a new markdown file\noutput_path = Path('YOUR/PATH/corrected_data.md')\nwith output_path.open('w') as out_file:\n    out_file.write(corrected_text)\n\n\n\nJan Kirenz"
  }
]